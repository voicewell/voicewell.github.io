<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Flyer | Voice of Wellness 2025</title>
    <script src="includes/common.js"></script>
</head>

<body>

    <div id="banner-container"></div>

    <div id="navigation-container"></div>

    <br>
    <h3> Dr. Felix Burkhardt</h3>
    <p>
    Dr. Felix Burkhardt does teaching, consulting, research and development with respect to speech based emotional human-machine interfaces.
 Originally an expert of Speech Synthesis at the Technical University of Berlin, he wrote his ph.d. thesis on the simulation of emotional speech by machines, recorded the Berlin acted emotions database, "EmoDB", and maintains several open source projects, including the emotional speech synthesizer Emofilt, the speech labeling and annotation tool Speechalyzer and the machine learning platform Nkululeko.
 Since 2018 he is the research director at audEERING after having worked for the Deutsche Telekom AG for 18 years. From 2019 to 2022 also was a full professor at the institute of communication science of the Technical University of Berlin.
    </p>
    <br/>Abstract:
    <p>In this lecture, we will explore the intersection of speech synthesis and mental wellness:</p>
    <ol>
        <li>Machine learning overview</li>
        <li>Evaluation metrics</li>
        <li>Nkululeko overview</li>
        <li>Classifiers for speech processing</li>
        <li>Speech features</li>
        <li>Splitting data</li>
        <li>Augmentation and feature normalization</li>
        <li>Cross database evaluation</li>
    </ol>
    Other themes includes: data preparation with audformat and CSV, bias detection, and random splicing of data.
    </p>
    <h3> Dr. Takenori Yoshimura</h3>
    <p>
        Takenori Yoshimura received his B.E. degree in Computer Science and Engineering and his M.E. and Ph.D. degrees in Scientific and Engineering Simulation from Nagoya Institute of Technology, Japan, in 2013, 2015, and 2018, respectively. From September 2015 to February 2016, he was a visiting researcher at the University of Edinburgh. He is currently a researcher at Nagoya Institute of Technology. His research interests include statistical machine learning and speech synthesis. He is a member of IEEE and the Acoustical Society of Japan.
    </p>
    <br/>Abstract:
    <p>The Speech Signal Processing Toolkit (SPTK) is an open-source suite of tools for speech signal processing, including speech analysis and synthesis. It has been actively maintained and widely used in the speech processing community since its initial release in 1998. This lecture introduces the core concepts of SPTK, along with a brief overview of the fundamentals of speech signal processing. In addition, a differentiable extension of SPTK, called diffsptk, is also introduced. Designed for integration with deep learning workflows, diffsptk helps bridge the gap between classical signal processing and modern neural network architectures. The lecture concludes with a hands-on session, where participants will learn how to use both SPTK and diffsptk through practical examples.</p>
    <h3> Dr. Bagus Tris Atmaja</h3>
    <p>researcher and lecturer in speech and audio processing. He received the B.E. and M.E. degrees from the Sepuluh Nopember Institute of Technology in 2009 and 2012, respectively, and the Ph.D. degree in information science with a focus on speech emotion recognition from the Japan Advanced Institute of Science and Technology in 2021. He has been employed as a Docent with the Vibrastic Laboratory, Sepuluh Nopember Institute of Technology, since 2014. He was also a Postdoctoral and a Project Researcher with the Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology (AIST). Currently, he is an assistant professor at Nara Institute of Science and Technology. His research interests include speech processing, affective computing, multimodal recognition, and multitask learning.</p>
    <br/>Abstract:
    <p>In this lecture, we will explore basic to intermediate concepts of speech processing, focusing on:</p>
    <ol>
        <li>Getting started with Git,</li>
        <li>Data balancing,</li>
        <li>Optimization,</li>
        <li>Ensemble learning,</li>
        <li>Pathological voice detection,</li>
        <li>Finding importance feature for dementia.</li>
    </ol>
    <h3> Dr. Dhany Arifianto (in confirmation)</h3>
    <!-- <table class="flyers">
        <tr>
            <td class="flyer">
                <a href="assets/MathConnections-Flyer.pdf">
                    <img src="assets/MathConnections-Flyer.png" alt="MathConnections Color Flyer" style="width:100%">
                </a>
            </td>
            <td class="flyer">
                <a href="assets/MathConnections-FlyerBW.pdf">
                    <img src="assets/MathConnections-FlyerBW.png" alt="MathConnections BW Flyer" style="width:100%">
                </a>
            </td>
        </tr>
    </table> -->
    <h2>References: </h2>
    <footer>
        &copy; Conference Organizers
        &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
    </footer>

</body>
</html>

