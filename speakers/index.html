<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Conference Template">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference">
    <title>Flyer | Voice of Wellness 2025</title>
    <script src="includes/common.js"></script>
</head>

<body>

    <div id="banner-container"></div>

    <div id="navigation-container"></div>

    <br>
    <h3> Dr. Felix Burkhardt</h3>
    <p>
    Dr. Felix Burkhardt does teaching, consulting, research and development with respect to speech based emotional human-machine interfaces.
 Originally an expert of Speech Synthesis at the Technical University of Berlin, he wrote his ph.d. thesis on the simulation of emotional speech by machines, recorded the Berlin acted emotions database, "EmoDB", and maintains several open source projects, including the emotional speech synthesizer Emofilt, the speech labeling and annotation tool Speechalyzer and the machine learning platform Nkululeko.
 Since 2018 he is the research director at audEERING after having worked for the Deutsche Telekom AG for 18 years. From 2019 to 2022 also was a full professor at the institute of communication science of the Technical University of Berlin.
    </p>
    <br/>Abstract:
    <p>In this lecture, I will explore the intersection of speech processing and machine learning speaker characteristics based on templates, particularly for mental wellnes. This workshop offers a comprehensive introduction to speech processing and voice analysis with a strong focus on practical machine learning applications. Participants will first be introduced to the fundamentals of machine learning and evaluation metrics, laying the groundwork for understanding model performance. The workshop continues with an overview of Nkululeko, a user-friendly toolkit tailored for rapid prototyping in speech-based machine learning tasks. Attendees will explore various classifiers commonly used in speech processing, including their strengths and limitations. Key topics such as speech feature extraction, data splitting strategies, augmentation techniques, and feature normalization will be discussed to enhance model generalizability and robustness. Finally, the workshop will address the challenges of cross-database evaluation, emphasizing the importance of building models that generalize across different recording conditions and speaker populations. This workshop is suitable for students and researchers entering the field of speech AI, as well as professionals seeking to update their practical skills.</p>
    <!-- <ol>
        <li>Machine learning overview</li>
        <li>Evaluation metrics</li>
        <li>Nkululeko overview</li>
        <li>Classifiers for speech processing</li>
        <li>Speech features</li>
        <li>Splitting data</li>
        <li>Augmentation and feature normalization</li>
        <li>Cross database evaluation</li>
    </ol>
    <p>Other themes includes: data preparation with audformat and CSV, bias detection, and random splicing of data.</p> -->
    </p>
    <h3> Dr. Takenori Yoshimura</h3>
    <p>
        Takenori Yoshimura received his B.E. degree in Computer Science and Engineering and his M.E. and Ph.D. degrees in Scientific and Engineering Simulation from Nagoya Institute of Technology, Japan, in 2013, 2015, and 2018, respectively. From September 2015 to February 2016, he was a visiting researcher at the University of Edinburgh. He is currently a researcher at Nagoya Institute of Technology. His research interests include statistical machine learning and speech synthesis. He is a member of IEEE and the Acoustical Society of Japan.
    </p>
    <br/>Abstract:
    <p>The Speech Signal Processing Toolkit (SPTK) is an open-source suite of tools for speech signal processing, including speech analysis and synthesis. It has been actively maintained and widely used in the speech processing community since its initial release in 1998. This lecture introduces the core concepts of SPTK, along with a brief overview of the fundamentals of speech signal processing. In addition, a differentiable extension of SPTK, called diffsptk, is also introduced. Designed for integration with deep learning workflows, diffsptk helps bridge the gap between classical signal processing and modern neural network architectures. The lecture concludes with a hands-on session, where participants will learn how to use both SPTK and diffsptk through practical examples.</p>
    <h3> Dr. Bagus Tris Atmaja</h3>
    <p>Bagus Tris Atmaja is a researcher and lecturer in speech and audio processing. He received the B.E. and M.E. degrees from the Sepuluh Nopember Institute of Technology in 2009 and 2012, respectively, and the Ph.D. degree in information science with a focus on speech emotion recognition from the Japan Advanced Institute of Science and Technology in 2021. He has been employed as a Docent with the Vibrastic Laboratory, Sepuluh Nopember Institute of Technology, since 2014. He was also a Postdoctoral and a Project Researcher with the Artificial Intelligence Research Center, National Institute of Advanced Industrial Science and Technology (AIST). Currently, he is an assistant professor at Nara Institute of Science and Technology. His research interests include speech processing, affective computing, multimodal recognition, and multitask learning.</p>
    <br/>Abstract:
    <!-- <p>In this lecture, we will explore basic to intermediate concepts of speech processing, focusing on:</p> -->
     <p>This tutorial offers a practical introduction to essential tools and techniques in speech and machine learning research. Participants will begin with foundational skills in version control using Git, followed by methods for improving model performance: data balancing, ensemble learning and model optimization. Applications in health-related speech analysis are highlighted through two case studies: pathological voice detection and the identification of important acoustic features associated with dementia. By the end, attendees will gain hands-on experience with tools and methods relevant to both general and specialized speech analysis tasks.</p>
    <!-- <ol>
        <li>Getting started with Git,</li>
        <li>Data balancing and optimization,</li>
        <li>Ensemble learning,</li>
        <li>Pathological voice detection,</li>
        <li>Finding importance feature for dementia.</li>
    </ol> -->
    <h3> Dr. Dhany Arifianto</h3>
    <!-- <table class="flyers">
        <tr>
            <td class="flyer">
                <a href="assets/MathConnections-Flyer.pdf">
                    <img src="assets/MathConnections-Flyer.png" alt="MathConnections Color Flyer" style="width:100%">
                </a>
            </td>
            <td class="flyer">
                <a href="assets/MathConnections-FlyerBW.pdf">
                    <img src="assets/MathConnections-FlyerBW.png" alt="MathConnections BW Flyer" style="width:100%">
                </a>
            </td>
        </tr>
    </table> -->
    <h2>References: </h2>
    <ol>
        <li>Burkhardt, F., Eyben, F., Schuller, B.W. (2023) Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming. Proc. Interspeech 2023, 2010-2011.</li>
        <li>Burkhardt, F., Atmaja, B. T., Derington, A., & Eyben, F. (2024). Check Your Audio Data: Nkululeko for Bias Detection. Oriental COCOSDA, 1-6. <a href="https://doi.org/10.1109/O-COCOSDA64382.2024.10800580">https://doi.org/10.1109/O-COCOSDA64382.2024.10800580</a></li>
        <li>Atmaja, B. T., Sasou, A., & Burkhardt, F. (2024). Uncertainty-Based Ensemble Learning for Speech Classification. 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-Ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA), 1-6. <a href="https://doi.org/10.1109/O-COCOSDA64382.2024.10800111">https://doi.org/10.1109/O-COCOSDA64382.2024.10800111</a></li>
        <li>Atmaja, B. T., & Sasou, A. (2025). Pathological Voice Detection From Sustained Vowels : Handcrafted vs. Self-supervised Learning. 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), 1-5. <a href="https://doi.org/10.1109/ICASSPW65056.2025.11011272">https://doi.org/10.1109/ICASSPW65056.2025.11011272</a></li>
        <li>Burkhardt, F., Derington, A., Kahlau, M., Scherer, K., Eyben, F., & Schuller, B. (2023). Masking Speech Contents by Random Splicing: is Emotional Expression Preserved? 1-5. <a href="https://doi.org/10.1109/icassp49357.2023.10097094">https://doi.org/10.1109/icassp49357.2023.10097094</a>.</li>
        <li>Yoshimura, T., Fujimoto, T., Oura, K., & Tokuda, K. (2023). SPTK4: An Open-Source Software Toolkit for Speech Signal Processing. 12th ISCA Speech Synthesis Workshop (SSW2023), August, 211-217. <a href="https://doi.org/10.21437/SSW.2023-33">https://doi.org/10.21437/SSW.2023-33</a></li>
    </ol>

    <footer>
        &copy; Conference Organizers
        &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
    </footer>

</body>
</html>

